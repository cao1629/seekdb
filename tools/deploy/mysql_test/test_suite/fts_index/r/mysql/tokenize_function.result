drop database if exists fts;
create database fts;
use fts;
set ob_query_timeout = 10000000000;
create table memo (id BIGINT AUTO_INCREMENT,title VARCHAR(100), body TEXT, FULLTEXT INDEX fti(title, body));
INSERT INTO memo (title, body) VALUES ('', '');
INSERT INTO memo (title, body) VALUES ('电影评论', '昨晚看的一部电影非常感人，让我思考了很多。');
INSERT INTO memo (title, body) VALUES ('音乐欣赏', '最近迷上了一位古典音乐家的作品，他的钢琴曲非常优美。');
INSERT INTO memo (title, body) VALUES ('旅行日记', '这次旅行中发生了很多有趣的事情，记录下来留作纪念。');
INSERT INTO memo (title, body) VALUES ('Security Audit', 'Completed the annual security audit. No major vulnerabilities were found, but we will implement additional measures for data protection.');
INSERT INTO memo (title, body) VALUES ('Employee Training', 'Scheduled training sessions for all employees on the latest industry standards and compliance requirements.');
INSERT INTO memo (title, body) VALUES ('Client Meeting', 'Prepared for the client meeting next week. We will present our progress and discuss future plans.');
INSERT INTO memo (title, body) VALUES ('Sales Report', 'Sales have increased by 15% compared to last quarter. The sales team is working on strategies to maintain this growth.');
result_format: 4

select tokenize('hello world');
+-------------------------+
| tokenize('hello world') |
+-------------------------+
| ["world", "hello"]      |
+-------------------------+
select tokenize("hello world");
+-------------------------+
| tokenize("hello world") |
+-------------------------+
| ["world", "hello"]      |
+-------------------------+
select tokenize('hello world', 'space');
+----------------------------------+
| tokenize('hello world', 'space') |
+----------------------------------+
| ["world", "hello"]               |
+----------------------------------+
select tokenize('hello world', 'beng');
+---------------------------------+
| tokenize('hello world', 'beng') |
+---------------------------------+
| ["world", "hello"]              |
+---------------------------------+
select tokenize('hello world', 'ngram');
+--------------------------------------------------+
| tokenize('hello world', 'ngram')                 |
+--------------------------------------------------+
| ["lo", "he", "ll", "or", "rl", "wo", "ld", "el"] |
+--------------------------------------------------+
select tokenize('hello world', 'space', '[{"output": "all"}]');
+---------------------------------------------------------+
| tokenize('hello world', 'space', '[{"output": "all"}]') |
+---------------------------------------------------------+
| {"tokens": [{"world": 1}, {"hello": 1}], "doc_len": 2}  |
+---------------------------------------------------------+
select tokenize('hello world', 'space', '[{"output": "default"}]');
+-------------------------------------------------------------+
| tokenize('hello world', 'space', '[{"output": "default"}]') |
+-------------------------------------------------------------+
| ["world", "hello"]                                          |
+-------------------------------------------------------------+

select tokenize("");
+--------------+
| tokenize("") |
+--------------+
| []           |
+--------------+
select tokenize('');
+--------------+
| tokenize('') |
+--------------+
| []           |
+--------------+

select tokenize('hello world', ' ngram');
+--------------------------------------------------+
| tokenize('hello world', ' ngram')                |
+--------------------------------------------------+
| ["lo", "he", "ll", "or", "rl", "wo", "ld", "el"] |
+--------------------------------------------------+
select tokenize('hello world', 'ngram ');
+--------------------------------------------------+
| tokenize('hello world', 'ngram ')                |
+--------------------------------------------------+
| ["lo", "he", "ll", "or", "rl", "wo", "ld", "el"] |
+--------------------------------------------------+

select tokenize('hello world', 'space', '[{"output": "all"}]', 'extra');
ERROR 42000: Incorrect parameter count in the call to native function 'tokenize'

select tokenize();
ERROR 42000: Incorrect parameter count in the call to native function 'tokenize'

select tokenize('hello world', '');
ERROR HY000: Invalid argument

select tokenize('hello world', ' ');
ERROR HY000: Invalid argument

select tokenize('hello world', 'ng ram');
ERROR HY000: Function ng ram is not defined

select tokenize('hello world', 'ngramm', '[{"output": "default"}]');
ERROR HY000: Function ngramm is not defined

select tokenize('hello world', 'ngra', '[{"output": "default"}]');
ERROR HY000: Function ngra is not defined

select tokenize('hello world', 'ngram', '[{"output": "defaults"}]');
ERROR HY000: Incorrect arguments to output mode should be string default or all

select tokenize('hello world', 'ngram', '[{"undefined": "all"}]');
ERROR HY000: Incorrect arguments to config: undefined

select tokenize('hello world', 'ngram', '[{"case": "upper"}]');
ERROR 0A000: case indentifier not supported

select tokenize('hello world', 'ngram', '[{"additional-args": ""}]');
ERROR HY000: Incorrect arguments to config: additional-args

select tokenize('hello world', 'ngram', '[{"stopwords": "abc,def"}]');
ERROR 0A000: stopwords not supported

select tokenize('hello world', 'ngram', '[{"output": "default"]');
ERROR 22032: Invalid JSON text in argument.

select tokenize(title, "ngram", "[{}]") as ttoken, tokenize(body, 'space', '[{"output": "all"}]') as btoken from memo;
+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ttoken                                                                         | btoken                                                                                                                                                                                                                                                         |
+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| []                                                                             | {"tokens": [], "doc_len": 0}                                                                                                                                                                                                                                   |
| ["影评", "电影", "评论"]                                                       | {"tokens": [{"让我思考了很多": 1}, {"昨晚看的一部电影非常感人": 1}], "doc_len": 2}                                                                                                                                                                             |
| ["音乐", "欣赏", "乐欣"]                                                       | {"tokens": [{"他的钢琴曲非常优美": 1}, {"最近迷上了一位古典音乐家的作品": 1}], "doc_len": 2}                                                                                                                                                                   |
| ["行日", "旅行", "日记"]                                                       | {"tokens": [{"这次旅行中发生了很多有趣的事情": 1}, {"记录下来留作纪念": 1}], "doc_len": 2}                                                                                                                                                                     |
| ["di", "se", "ty", "ri", "it", "au", "ud", "ur", "ec", "cu"]                   | {"tokens": [{"implement": 1}, {"additional": 1}, {"were": 1}, {"data": 1}, {"measures": 1}, {"audit": 1}, {"but": 1}, {"found": 1}, {"completed": 1}, {"security": 1}, {"major": 1}, {"protection": 1}, {"vulnerabilities": 1}, {"annual": 1}], "doc_len": 14} |
| ["lo", "em", "pl", "ni", "in", "tr", "ee", "ye", "mp", "oy", "ng", "ra", "ai"] | {"tokens": [{"compliance": 1}, {"industry": 1}, {"scheduled": 1}, {"employees": 1}, {"all": 1}, {"requirements": 1}, {"latest": 1}, {"training": 1}, {"sessions": 1}, {"standards": 1}, {"and": 1}], "doc_len": 11}                                            |
| ["ie", "in", "me", "ee", "et", "nt", "ti", "en", "ng", "cl", "li"]             | {"tokens": [{"present": 1}, {"meeting": 1}, {"prepared": 1}, {"client": 1}, {"progress": 1}, {"next": 1}, {"future": 1}, {"plans": 1}, {"and": 1}, {"week": 1}, {"our": 1}, {"discuss": 1}], "doc_len": 12}                                                    |
| ["po", "le", "re", "ep", "or", "al", "es", "rt", "sa"]                         | {"tokens": [{"team": 1}, {"have": 1}, {"last": 1}, {"maintain": 1}, {"compared": 1}, {"strategies": 1}, {"increased": 1}, {"working": 1}, {"quarter": 1}, {"sales": 2}, {"growth": 1}], "doc_len": 12}                                                         |
+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

select * from memo where '电影' member of (tokenize(title, "ngram")) OR 'growth' member of (tokenize(body, "space"));
+----+--------------+------------------------------------------------------------------------------------------------------------------------+
| id | title        | body                                                                                                                   |
+----+--------------+------------------------------------------------------------------------------------------------------------------------+
|  2 | 电影评论     | 昨晚看的一部电影非常感人，让我思考了很多。                                                                             |
|  8 | Sales Report | Sales have increased by 15% compared to last quarter. The sales team is working on strategies to maintain this growth. |
+----+--------------+------------------------------------------------------------------------------------------------------------------------+

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 0}]}]');
ERROR HY000: Incorrect arguments to the ngram_token_size must be in [1, 10]

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 1}]}]');
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 1}]}]')                                                                                                                                                              |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {"tokens": [{"f": 1}, {"x": 1}, {"j": 1}, {"s": 1}, {"a": 1}, {"q": 1}, {"h": 2}, {"c": 1}, {"o": 3}, {"l": 4}, {"u": 1}, {"g": 1}, {"n": 1}, {"t": 1}, {"y": 1}, {"i": 1}, {"r": 2}, {"b": 1}, {"w": 2}, {"k": 1}, {"e": 2}, {"v": 1}, {"d": 2}, {"z": 1}, {"p": 1}, {"m": 1}], "doc_len": 36} |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 2}]}]');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 2}]}]')                                                                                                                                                                                                                                                                     |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {"tokens": [{"ab": 1}, {"st": 1}, {"vw": 1}, {"lo": 1}, {"hi": 1}, {"ef": 1}, {"gh": 1}, {"xy": 1}, {"yz": 1}, {"mn": 1}, {"he": 1}, {"tu": 1}, {"fg": 1}, {"ll": 1}, {"rs": 1}, {"lm": 1}, {"op": 1}, {"or": 1}, {"de": 1}, {"rl": 1}, {"ij": 1}, {"pq": 1}, {"uv": 1}, {"no": 1}, {"wo": 1}, {"kl": 1}, {"ld": 1}, {"jk": 1}, {"bc": 1}, {"wx": 1}, {"qr": 1}, {"cd": 1}, {"el": 1}], "doc_len": 33} |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 10}]}]');
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 10}]}]')                                                                                                                                                                                                                            |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {"tokens": [{"fghijklmno": 1}, {"efghijklmn": 1}, {"qrstuvwxyz": 1}, {"defghijklm": 1}, {"bcdefghijk": 1}, {"jklmnopqrs": 1}, {"ghijklmnop": 1}, {"pqrstuvwxy": 1}, {"klmnopqrst": 1}, {"abcdefghij": 1}, {"nopqrstuvw": 1}, {"lmnopqrstu": 1}, {"opqrstuvwx": 1}, {"hijklmnopq": 1}, {"cdefghijkl": 1}, {"mnopqrstuv": 1}, {"ijklmnopqr": 1}], "doc_len": 17} |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ngram', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 11}]}]');
ERROR HY000: Incorrect arguments to the ngram_token_size must be in [1, 10]

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 5}]}]');
+----------------------------------------------------------------------------------------------------------------------------------+
| tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 5}]}]') |
+----------------------------------------------------------------------------------------------------------------------------------+
| {"tokens": [{"world": 1}, {"hello": 1}, {"abcdefghijklmnopqrstuvwxyz": 1}], "doc_len": 3}                                        |
+----------------------------------------------------------------------------------------------------------------------------------+
select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 6}]}]');
+----------------------------------------------------------------------------------------------------------------------------------+
| tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 6}]}]') |
+----------------------------------------------------------------------------------------------------------------------------------+
| {"tokens": [{"abcdefghijklmnopqrstuvwxyz": 1}], "doc_len": 1}                                                                    |
+----------------------------------------------------------------------------------------------------------------------------------+

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 5}, {"max_token_size": 9}]}]');
ERROR HY000: Incorrect arguments to the max_token_size must be in [10, 84]

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 5}, {"max_token_size": 10}]}]');
+----------------------------------------------------------------------------------------------------------------------------------------------------------+
| tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 5}, {"max_token_size": 10}]}]') |
+----------------------------------------------------------------------------------------------------------------------------------------------------------+
| {"tokens": [{"world": 1}, {"hello": 1}], "doc_len": 2}                                                                                                   |
+----------------------------------------------------------------------------------------------------------------------------------------------------------+

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 11}, {"max_token_size": 10}]}]');
ERROR HY000: Incorrect arguments to the max_token_size must be equal to or greater than min_token_size

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 0}]}]');
ERROR HY000: Incorrect arguments to the min_token_size must be in [1, 16]

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"min_token_size": 17}]}]');
ERROR HY000: Incorrect arguments to the min_token_size must be in [1, 16]

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"not_existed_name": 11}, {"max_token_size": 10}]}]');
ERROR 0A000: space config not supported

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'space', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 6}, {"max_token_size": 10}]}]');
ERROR 0A000: space config not supported

select tokenize('hello world abcdefghijklmnopqrstuvwxyz', 'ik', '[{"output": "all"}, {"additional_args": [{"ngram_token_size": 6}, {"max_token_size": 10}]}]');
ERROR 0A000: ik config not supported

drop table if exists memo;
drop database fts;
